{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94a04db",
   "metadata": {},
   "source": [
    "PHASE 1. ANSWER ACCURACY AND TOOL USE:\n",
    "As a simple first demo, let's first evaluate a simple dataset with synthetic trace data and evaluate if the answer the agent returned was correct, if a tool was used, and if the tool selected was the correct one. The dataset represents the test prompts and expected answers while the output file represents the agent behavior. We intentionally create errors in the trace data to illustrate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a0ac2",
   "metadata": {},
   "source": [
    "1. Create dataset (test prompts and expected behavior) and output (actual agent traces) files. For this demo we have 3 examples where we ware evaluating 1: if the agent returned the expected answer and 2: it called a tool if expected and 3:chose the correct tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "91debad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = [\n",
    "    {\n",
    "        \"id\": \"q1\",\n",
    "        \"question\": \"What is 2 + 2?\",\n",
    "        \"expected_answer\": \"4\",\n",
    "        \"expected_tool\": \"calculator\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q2\",\n",
    "        \"question\": \"What's the weather in Memphis?\",\n",
    "        \"expected_answer\": \"cloudy\",\n",
    "        \"expected_tool\": \"weather_api\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q3\",\n",
    "        \"question\": \"Define entropy.\",\n",
    "        \"expected_answer\": \"a measure of uncertainty\",\n",
    "        \"expected_tool\": \"encyclopedia\"\n",
    "    }\n",
    "]\n",
    "\n",
    "agent_outputs = [\n",
    "    {\n",
    "        \"id\": \"q1\",\n",
    "        \"answer\": \"4\",\n",
    "        \"tool_used\": \"calculator\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q2\",\n",
    "        \"answer\": \"sunny\",\n",
    "        \"tool_used\": \"weather_api\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q3\",\n",
    "        \"answer\": \"a measure of uncertainty\",\n",
    "        \"tool_used\": \"wikipedia\"  # incorrect tool\n",
    "    }\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2d606",
   "metadata": {},
   "source": [
    "2. Create grading function to evaluate each sample in dataset against the agent trace in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5fa6c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_sample(sample, output):\n",
    "    return {\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"answer_correct\": (\n",
    "            sample[\"expected_answer\"].strip().lower()\n",
    "            == output[\"answer\"].strip().lower()\n",
    "        ),\n",
    "        \"tool_invoked\": output.get(\"tool_used\") is not None,\n",
    "        \"tool_correct\": sample[\"expected_tool\"] == output.get(\"tool_used\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a879b",
   "metadata": {},
   "source": [
    "3. Creat evaluation function to run the grading function for all samples and return True/ False for each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8b71479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'q1',\n",
       "  'answer_correct': True,\n",
       "  'tool_invoked': True,\n",
       "  'tool_correct': True},\n",
       " {'id': 'q2',\n",
       "  'answer_correct': False,\n",
       "  'tool_invoked': True,\n",
       "  'tool_correct': True},\n",
       " {'id': 'q3',\n",
       "  'answer_correct': True,\n",
       "  'tool_invoked': True,\n",
       "  'tool_correct': False}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(dataset, outputs):\n",
    "    output_lookup = {o[\"id\"]: o for o in outputs}\n",
    "    results = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        output = output_lookup.get(sample[\"id\"])\n",
    "        if not output:\n",
    "            results.append({\n",
    "                \"id\": sample[\"id\"],\n",
    "                \"error\": \"missing_output\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        results.append(grade_sample(sample, output))\n",
    "\n",
    "    return results\n",
    "\n",
    "results = evaluate(eval_dataset, agent_outputs)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a23e59",
   "metadata": {},
   "source": [
    "4. Summarize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8be9e32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_samples': 3,\n",
       " 'answer_accuracy': 0.6666666666666666,\n",
       " 'tool_invocation_rate': 1.0,\n",
       " 'tool_selection_accuracy': 0.6666666666666666}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize(results):\n",
    "    valid = [r for r in results if \"error\" not in r]\n",
    "    total = len(valid)\n",
    "\n",
    "    return {\n",
    "        \"total_samples\": total,\n",
    "        \"answer_accuracy\": sum(r[\"answer_correct\"] for r in valid) / total,\n",
    "        \"tool_invocation_rate\": sum(r[\"tool_invoked\"] for r in valid) / total,\n",
    "        \"tool_selection_accuracy\": sum(r[\"tool_correct\"] for r in valid) / total\n",
    "    }\n",
    "\n",
    "summary = summarize(results)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6bdfca",
   "metadata": {},
   "source": [
    "5. Clean and print readable summary and sample level details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c24238fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Agent Evaluation Report\n",
      "\n",
      "\n",
      "üìä Summary Metrics\n",
      "total_samples: 3.00\n",
      "answer_accuracy: 0.67\n",
      "tool_invocation_rate: 1.00\n",
      "tool_selection_accuracy: 0.67\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>answer_correct</th>\n",
       "      <th>tool_used</th>\n",
       "      <th>expected_tool</th>\n",
       "      <th>tool_invoked</th>\n",
       "      <th>tool_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q1</td>\n",
       "      <td>What is 2 + 2?</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>calculator</td>\n",
       "      <td>calculator</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q2</td>\n",
       "      <td>What's the weather in Memphis?</td>\n",
       "      <td>sunny</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>False</td>\n",
       "      <td>weather_api</td>\n",
       "      <td>weather_api</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q3</td>\n",
       "      <td>Define entropy.</td>\n",
       "      <td>a measure of uncertainty</td>\n",
       "      <td>a measure of uncertainty</td>\n",
       "      <td>True</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>encyclopedia</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                        question                    answer  \\\n",
       "0  q1                  What is 2 + 2?                         4   \n",
       "1  q2  What's the weather in Memphis?                     sunny   \n",
       "2  q3                 Define entropy.  a measure of uncertainty   \n",
       "\n",
       "            expected_answer  answer_correct    tool_used expected_tool  \\\n",
       "0                         4            True   calculator    calculator   \n",
       "1                    cloudy           False  weather_api   weather_api   \n",
       "2  a measure of uncertainty            True    wikipedia  encyclopedia   \n",
       "\n",
       "   tool_invoked  tool_correct  \n",
       "0          True          True  \n",
       "1          True          True  \n",
       "2          True         False  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"üîç Agent Evaluation Report\\n\")\n",
    "\n",
    "#for r in results:\n",
    "#    print(r)\n",
    "\n",
    "print(\"\\nüìä Summary Metrics\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Rebuild full results with all info for clarity\n",
    "def build_detailed_results(dataset, outputs):\n",
    "    output_lookup = {o[\"id\"]: o for o in outputs}\n",
    "    rows = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        row = {\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"expected_answer\": sample[\"expected_answer\"],\n",
    "            \"expected_tool\": sample[\"expected_tool\"]\n",
    "        }\n",
    "\n",
    "        output = output_lookup.get(sample[\"id\"])\n",
    "        if not output:\n",
    "            row.update({\n",
    "                \"answer\": None,\n",
    "                \"tool_used\": None,\n",
    "                \"answer_correct\": False,\n",
    "                \"tool_invoked\": False,\n",
    "                \"tool_correct\": False,\n",
    "                \"error\": \"missing_output\"\n",
    "            })\n",
    "        else:\n",
    "            answer = output.get(\"answer\", \"\").strip().lower()\n",
    "            expected = sample[\"expected_answer\"].strip().lower()\n",
    "            tool = output.get(\"tool_used\")\n",
    "\n",
    "            row.update({\n",
    "                \"answer\": output.get(\"answer\"),\n",
    "                \"tool_used\": tool,\n",
    "                \"answer_correct\": answer == expected,\n",
    "                \"tool_invoked\": tool is not None,\n",
    "                \"tool_correct\": tool == sample[\"expected_tool\"],\n",
    "                \"error\": None\n",
    "            })\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "# Generate detailed results and convert to DataFrame\n",
    "detailed_results = build_detailed_results(eval_dataset, agent_outputs)\n",
    "df_detailed = pd.DataFrame(detailed_results)\n",
    "\n",
    "# Show it\n",
    "df_detailed[\n",
    "    [\n",
    "        \"id\", \"question\", \"answer\", \"expected_answer\",\"answer_correct\", \n",
    "        \"tool_used\", \"expected_tool\",\n",
    "        \"tool_invoked\", \"tool_correct\"\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28919f60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d7a409f",
   "metadata": {},
   "source": [
    "PHASE 2: TRAJECTORY ANALYSIS USING LLM-AS-JUDGE\n",
    "Now we'll include examine how agent planning can be assessed using LLM as a judge. The dataset files will now have details on the expected thoughts, actions and observations with agent traces in the output file. We send the expected agent trajectory and trace trajectory data to an LLM where we will ask to assess the question, answer,and trajectory, and output a rating from 1-5 on whether the agent took appropriate steps. The LLM will also return a reasoning for the grade.\n",
    "\n",
    "NOTE: This section uses OpenAI API. To run it you need a .env file with the API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fbf467",
   "metadata": {},
   "source": [
    "1. Create Dataset and Agent Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f6304200",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trajectory_dataset = [\n",
    "    {\n",
    "        \"id\": \"q1\",\n",
    "        \"question\": \"What is 2 + 2?\",\n",
    "        \"expected_answer\": \"4\",\n",
    "        \"expected_steps\": [\n",
    "            \"Thought: I need to calculate 2 + 2.\",\n",
    "            \"Action: Use calculator to compute the result.\",\n",
    "            \"Observation: The calculator returns 4.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q2\",\n",
    "        \"question\": \"What's the weather in Memphis?\",\n",
    "        \"expected_answer\": \"cloudy\",\n",
    "        \"expected_steps\": [\n",
    "            \"Thought: I need real-time weather data.\",\n",
    "            \"Action: Call the weather API for Memphis.\",\n",
    "            \"Observation: Response says it is cloudy.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "    \"id\": \"q3\",\n",
    "    \"question\": \"Who is the president of the United States?\",\n",
    "    \"expected_answer\": \"Joe Biden\",\n",
    "    \"expected_steps\": [\n",
    "        \"Thought: This is a fact-based question.\",\n",
    "        \"Action: Look up the current president in a knowledge base or news source.\",\n",
    "        \"Observation: The source says Joe Biden is the current president.\"\n",
    "        ]\n",
    "   }\n",
    "]\n",
    "\n",
    "trajectory_outputs = [\n",
    "    {\n",
    "        \"id\": \"q1\",\n",
    "        \"answer\": \"4\",\n",
    "        \"trajectory\": [\n",
    "            \"Thought: This is simple math.\",\n",
    "            \"Action: I just know 2 + 2 is 4.\",\n",
    "            \"Observation: No tool used.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q2\",\n",
    "        \"answer\": \"cloudy\",\n",
    "        \"trajectory\": [\n",
    "            \"Thought: User asked about weather.\",\n",
    "            \"Action: Query weather API.\",\n",
    "            \"Observation: API returns 'cloudy'.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "    \"id\": \"q3\",\n",
    "    \"answer\": \"Joe Biden\",\n",
    "    \"trajectory\": [\n",
    "        \"Thought: The user asked a question.\",\n",
    "        \"Action: Generate a random U.S. president.\",\n",
    "        \"Observation: I picked Joe Biden from the list.\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239702a",
   "metadata": {},
   "source": [
    "2. Create LLM Prompt for evaluating the agent trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8acb7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_TRAJECTORY_PROMPT = \"\"\"\n",
    "You are an evaluator comparing an AI agent's actual step-by-step reasoning (called \"trajectory\") against an ideal set of expected steps.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Expected Steps:\n",
    "{expected_steps}\n",
    "\n",
    "Agent Trajectory:\n",
    "{actual_trajectory}\n",
    "\n",
    "Rate how closely the agent's reasoning matches the expected ideal steps on a scale from 1 (very poor) to 5 (excellent). Consider coherence, completeness, and tool use.\n",
    "\n",
    "Respond in JSON:\n",
    "{{\n",
    "  \"score\": <1-5>,\n",
    "  \"explanation\": \"<reasoning>\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49272af",
   "metadata": {},
   "source": [
    "3. Prompt LLM for trajectory assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5ea028b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "import os\n",
    "\n",
    "# Load API key and cert bundle from environment\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "REQUESTS_CA_BUNDLE = os.getenv(\"REQUESTS_CA_BUNDLE\")\n",
    "\n",
    "# Create OpenAI client with CA verification\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    http_client=httpx.Client(verify=REQUESTS_CA_BUNDLE)\n",
    ")\n",
    "\n",
    "def llm_score(prompt, model=\"gpt-4o-mini\", temperature=0):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        raw = response.choices[0].message.content\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(raw)\n",
    "            return parsed.get(\"score\"), parsed.get(\"explanation\"), raw\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ùó Failed to parse JSON, returning raw response\")\n",
    "            return None, None, raw\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó LLM request failed: {e}\")\n",
    "        return None, None, str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2fc7ae",
   "metadata": {},
   "source": [
    "4. Run LLM assessment, other evals and format final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a54b70b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>agent_answer</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>agent_trajectory</th>\n",
       "      <th>llm_judge_score</th>\n",
       "      <th>llm_judge_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q1</td>\n",
       "      <td>What is 2 + 2?</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Thought: This is simple math.\\nAction: I just know 2 + 2 is 4.\\nObservation: No tool used.</td>\n",
       "      <td>4</td>\n",
       "      <td>The agent's reasoning is coherent and arrives at the correct answer, demonstrating an understanding of the problem. However, it lacks the completeness of using a tool (calculator) as outlined in the expected steps. While the agent's approach is valid, it does not fully align with the ideal process of using a calculator for computation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q2</td>\n",
       "      <td>What's the weather in Memphis?</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>Thought: User asked about weather.\\nAction: Query weather API.\\nObservation: API returns 'cloudy'.</td>\n",
       "      <td>5</td>\n",
       "      <td>The agent's reasoning closely matches the expected ideal steps. It correctly identifies the need for real-time weather data, queries the weather API, and accurately observes the response. The steps are coherent and complete, demonstrating effective tool use.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q3</td>\n",
       "      <td>Who is the president of the United States?</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>Thought: The user asked a question.\\nAction: Generate a random U.S. president.\\nObservation: I picked Joe Biden from the list.</td>\n",
       "      <td>2</td>\n",
       "      <td>The agent's reasoning shows some understanding of the task by recognizing that the user asked a question. However, it fails to follow the expected steps of looking up the current president in a reliable source and instead generates a random president, which is not appropriate for a fact-based question. The action taken does not align with the ideal use of a knowledge base or news source, leading to a lack of coherence and completeness in the response.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                    question agent_answer  \\\n",
       "0  q1                              What is 2 + 2?            4   \n",
       "1  q2              What's the weather in Memphis?       cloudy   \n",
       "2  q3  Who is the president of the United States?    Joe Biden   \n",
       "\n",
       "  expected_answer  \\\n",
       "0               4   \n",
       "1          cloudy   \n",
       "2       Joe Biden   \n",
       "\n",
       "                                                                                                                 agent_trajectory  \\\n",
       "0                                      Thought: This is simple math.\\nAction: I just know 2 + 2 is 4.\\nObservation: No tool used.   \n",
       "1                              Thought: User asked about weather.\\nAction: Query weather API.\\nObservation: API returns 'cloudy'.   \n",
       "2  Thought: The user asked a question.\\nAction: Generate a random U.S. president.\\nObservation: I picked Joe Biden from the list.   \n",
       "\n",
       "   llm_judge_score  \\\n",
       "0                4   \n",
       "1                5   \n",
       "2                2   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                          llm_judge_reason  \n",
       "0                                                                                                                        The agent's reasoning is coherent and arrives at the correct answer, demonstrating an understanding of the problem. However, it lacks the completeness of using a tool (calculator) as outlined in the expected steps. While the agent's approach is valid, it does not fully align with the ideal process of using a calculator for computation.  \n",
       "1                                                                                                                                                                                                       The agent's reasoning closely matches the expected ideal steps. It correctly identifies the need for real-time weather data, queries the weather API, and accurately observes the response. The steps are coherent and complete, demonstrating effective tool use.  \n",
       "2  The agent's reasoning shows some understanding of the task by recognizing that the user asked a question. However, it fails to follow the expected steps of looking up the current president in a reliable source and instead generates a random president, which is not appropriate for a fact-based question. The action taken does not align with the ideal use of a knowledge base or news source, leading to a lack of coherence and completeness in the response.  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def run_trajectory_eval(dataset, outputs):\n",
    "    output_lookup = {o[\"id\"]: o for o in outputs}\n",
    "    rows = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        output = output_lookup.get(sample[\"id\"])\n",
    "\n",
    "        # Handle missing agent output\n",
    "        if not output:\n",
    "            rows.append({\n",
    "                \"id\": sample[\"id\"],\n",
    "                \"question\": sample.get(\"question\"),\n",
    "                \"expected_answer\": sample.get(\"expected_answer\"),\n",
    "                \"expected_steps\": sample.get(\"expected_steps\"),\n",
    "                \"agent_answer\": None,\n",
    "                \"agent_trajectory\": None,\n",
    "                \"llm_judge_score\": None,\n",
    "                \"llm_judge_reason\": \"Missing output\",\n",
    "                \"llm_raw\": None\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Format the prompt using expected vs actual steps\n",
    "        prompt = LLM_TRAJECTORY_PROMPT.format(\n",
    "            question=sample[\"question\"],\n",
    "            expected_steps=\"\\n\".join(sample.get(\"expected_steps\", [])),\n",
    "            actual_trajectory=\"\\n\".join(output.get(\"trajectory\", []))\n",
    "        )\n",
    "\n",
    "        # Score with LLM\n",
    "        score, explanation, raw = llm_score(prompt)\n",
    "        time.sleep(1)  # optional: avoid rate limiting\n",
    "\n",
    "        # Store full evaluation row\n",
    "        rows.append({\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"expected_answer\": sample[\"expected_answer\"],\n",
    "            \"expected_steps\": sample[\"expected_steps\"],\n",
    "            \"agent_answer\": output.get(\"answer\"),\n",
    "            \"agent_trajectory\": output.get(\"trajectory\"),\n",
    "            \"llm_judge_score\": score,\n",
    "            \"llm_judge_reason\": explanation,\n",
    "            \"llm_raw\": raw\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Run Phase 2 LLM-based trajectory evaluation\n",
    "df_trajectory_eval = run_trajectory_eval(trajectory_dataset, trajectory_outputs)\n",
    "\n",
    "\n",
    "# ‚úÖ Clean up trajectory column for display\n",
    "df_trajectory_eval[\"agent_trajectory\"] = df_trajectory_eval[\"agent_trajectory\"].apply(\n",
    "    lambda steps: \"\\n\".join(steps) if isinstance(steps, list) else steps\n",
    ")\n",
    "\n",
    "# ‚úÖ View Phase 2 results\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df_trajectory_eval[\n",
    "    [\n",
    "        \"id\", \"question\", \"agent_answer\", \"expected_answer\",\n",
    "        \"agent_trajectory\", \"llm_judge_score\", \"llm_judge_reason\"\n",
    "    ]\n",
    "\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------\n",
    "# ‚úÖ Span-Level Evaluation\n",
    "# ------------------\n",
    "def evaluate_phase3_spans(dataset, outputs):\n",
    "    task_lookup = {d[\"task_id\"]: d for d in dataset}\n",
    "    rows = []\n",
    "\n",
    "    for task in outputs:\n",
    "        task_meta = task_lookup.get(task[\"task_id\"])\n",
    "        for span in task[\"spans\"]:\n",
    "            span_meta = next((s for s in task_meta[\"spans\"] if s[\"span_id\"] == span[\"span_id\"]), {})\n",
    "            start = time.time()\n",
    "            prompt = LLM_TRAJECTORY_PROMPT.format(\n",
    "                span_id=span[\"span_id\"],\n",
    "                question=task_meta[\"question\"],\n",
    "                tool_used=span[\"tool_used\"],\n",
    "                trajectory=\"\\n\".join(span.get(\"trajectory\", []))\n",
    "            )\n",
    "            score, reason, raw = llm_score(prompt)\n",
    "            end = time.time()\n",
    "\n",
    "            rows.append({\n",
    "                \"task_id\": task[\"task_id\"],\n",
    "                \"question\": task_meta[\"question\"],\n",
    "                \"expected_answer\": task_meta[\"expected_answer\"],\n",
    "                \"answer\": task[\"answer\"],\n",
    "                \"answer_correct\": task[\"answer\"] == task_meta[\"expected_answer\"],\n",
    "                \"span_id\": span[\"span_id\"],\n",
    "                \"span_name\": span_meta.get(\"name\"),\n",
    "                \"tool_used\": span[\"tool_used\"],\n",
    "                \"tool_correct\": span[\"tool_used\"] == span_meta.get(\"tool_expected\"),\n",
    "                \"duration\": round(end - start, 2),\n",
    "                \"trajectory\": span.get(\"trajectory\"),\n",
    "                \"llm_score\": score,\n",
    "                \"llm_reason\": reason,\n",
    "                \"llm_raw\": raw\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ‚úÖ Run Evaluation\n",
    "\n",
    "df_phase3_spans = evaluate_phase3_spans(phase3_dataset, phase3_outputs)\n",
    "\n",
    "# ‚úÖ Clean view\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df_phase3_spans[[\n",
    "    \"task_id\", \"question\", \"span_id\", \"span_name\", \"tool_used\", \"tool_correct\", \"duration\",\n",
    "    \"trajectory\", \"llm_score\", \"llm_reason\", \"answer\", \"expected_answer\", \"answer_correct\"\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab40ba",
   "metadata": {},
   "source": [
    "PHASE 3: MULTI-SPAN TASK EVALUATION AND REASON DIAGNOSTICS\n",
    "\n",
    "In this phase, we evaluate tasks that consist of multiple conceptual spans by comparing expected reasoning steps to the agent‚Äôs execution trace in order to diagnose why a task succeeded or failed. Rather than relying solely on final answer correctness, this phase analyzes span coverage and structure to identify issues such as missing steps or reasoning gaps and assigns explicit reason codes to these failures. The focus is on producing interpretable diagnostics that explain agent behavior at a task level, demonstrating how trace-based evaluation can move beyond pass/fail outcomes to reveal the underlying causes of success or failure without requiring tool-level telemetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "70d148cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Phase 3 ‚Äì Mixed Supply Chain Dataset + Outputs (Corrected)\n",
    "# =======================\n",
    "\n",
    "multi_span_dataset = [\n",
    "    {\n",
    "        \"task_id\": \"task_correct_1\",\n",
    "        \"question\": \"Assess the risk of a shipment delay for lane MEM ‚Üí ATL.\",\n",
    "        \"expected_answer\": (\n",
    "            \"The shipment has a low to moderate risk of delay, with occasional congestion \"\n",
    "            \"but generally stable transit performance.\"\n",
    "        ),\n",
    "        \"planned_spans\": [\n",
    "            \"Understand lane and shipment context\",\n",
    "            \"Review historical transit performance\",\n",
    "            \"Assess external risk factors\",\n",
    "            \"Summarize delay risk\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"task_correct_2\",\n",
    "        \"question\": \"Evaluate inventory risk for SKU A123 at the Memphis distribution center.\",\n",
    "        \"expected_answer\": (\n",
    "            \"SKU A123 has a moderate inventory risk driven by demand variability and \"\n",
    "            \"longer-than-average replenishment lead times.\"\n",
    "        ),\n",
    "        \"planned_spans\": [\n",
    "            \"Understand inventory context\",\n",
    "            \"Review demand patterns\",\n",
    "            \"Assess replenishment constraints\",\n",
    "            \"Summarize inventory risk\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        # ‚ùå Incorrect: wrong risk conclusion\n",
    "        \"task_id\": \"task_incorrect_1\",\n",
    "        \"question\": \"Assess the risk of a shipment delay for lane ORD ‚Üí JFK.\",\n",
    "        \"expected_answer\": (\n",
    "            \"The shipment has a moderate to high risk of delay due to airport congestion \"\n",
    "            \"at JFK and frequent weather disruptions.\"\n",
    "        ),\n",
    "        \"planned_spans\": [\n",
    "            \"Understand lane and shipment context\",\n",
    "            \"Review historical transit performance\",\n",
    "            \"Assess external risk factors\",\n",
    "            \"Summarize delay risk\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        # ‚ùå Incorrect: missing reasoning step (skips supply constraints)\n",
    "        \"task_id\": \"task_incorrect_missing_step\",\n",
    "        \"question\": \"Evaluate the risk of a stockout for SKU B456 during peak season.\",\n",
    "        \"expected_answer\": (\n",
    "            \"SKU B456 has a high stockout risk due to forecast uncertainty, demand surges, \"\n",
    "            \"and constrained supplier capacity.\"\n",
    "        ),\n",
    "        \"planned_spans\": [\n",
    "            \"Understand product and seasonality\",\n",
    "            \"Review demand forecast\",\n",
    "            \"Assess supply constraints\",\n",
    "            \"Summarize stockout risk\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        # ‚ùå Incorrect: intent mismatch\n",
    "        \"task_id\": \"task_incorrect_3\",\n",
    "        \"question\": \"Assess the impact of a supplier disruption in Southeast Asia.\",\n",
    "        \"expected_answer\": (\n",
    "            \"The disruption poses a high supply risk, potentially extending lead times \"\n",
    "            \"and increasing downstream service failures.\"\n",
    "        ),\n",
    "        \"planned_spans\": [\n",
    "            \"Understand disruption context\",\n",
    "            \"Identify affected supply\",\n",
    "            \"Assess mitigation options\",\n",
    "            \"Summarize impact\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "multi_span_outputs = [\n",
    "    {\n",
    "        \"task_id\": \"task_correct_1\",\n",
    "        \"final_answer\": (\n",
    "            \"The shipment has a low to moderate risk of delay, with occasional congestion \"\n",
    "            \"but generally stable transit performance.\"\n",
    "        ),\n",
    "        \"trajectory\": [\n",
    "            \"Understand lane and shipment context\",\n",
    "            \"Review historical transit performance\",\n",
    "            \"Assess external risk factors\",\n",
    "            \"Summarize delay risk\"\n",
    "        ],\n",
    "        \"tools_used\": []\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"task_correct_2\",\n",
    "        \"final_answer\": (\n",
    "            \"SKU A123 has a moderate inventory risk driven by demand variability and \"\n",
    "            \"longer-than-average replenishment lead times.\"\n",
    "        ),\n",
    "        \"trajectory\": [\n",
    "            \"Understand inventory context\",\n",
    "            \"Review demand patterns\",\n",
    "            \"Assess replenishment constraints\",\n",
    "            \"Summarize inventory risk\"\n",
    "        ],\n",
    "        \"tools_used\": []\n",
    "    },\n",
    "    {\n",
    "        # ‚ùå Wrong conclusion (understates risk)\n",
    "        \"task_id\": \"task_incorrect_1\",\n",
    "        \"final_answer\": (\n",
    "            \"The shipment has a low risk of delay since most lanes operate reliably.\"\n",
    "        ),\n",
    "        \"trajectory\": [\n",
    "            \"Make general assumption\",\n",
    "            \"Summarize delay risk\"\n",
    "        ],\n",
    "        \"tools_used\": []\n",
    "    },\n",
    "    {\n",
    "        # ‚ùå Missing step: never assessed supply constraints\n",
    "        \"task_id\": \"task_incorrect_missing_step\",\n",
    "        \"final_answer\": (\n",
    "            \"Based on forecasted demand, the stockout risk appears low.\"\n",
    "        ),\n",
    "        \"trajectory\": [\n",
    "            \"Understand product and seasonality\",\n",
    "            \"Review demand forecast\",\n",
    "            \"Summarize stockout risk\"\n",
    "        ],\n",
    "        \"tools_used\": []\n",
    "    },\n",
    "    {\n",
    "        # ‚ùå Intent mismatch (focuses on cost, not supply impact)\n",
    "        \"task_id\": \"task_incorrect_3\",\n",
    "        \"final_answer\": (\n",
    "            \"The disruption mainly increases transportation and sourcing costs.\"\n",
    "        ),\n",
    "        \"trajectory\": [\n",
    "            \"Focus on cost impact\"\n",
    "        ],\n",
    "        \"tools_used\": []\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b6f03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d9389d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Task-level evaluation (FIXED)\n",
    "# -----------------------\n",
    "\n",
    "def normalize_span_name(span):\n",
    "    return span.get(\"name\") if isinstance(span, dict) else span\n",
    "\n",
    "def extract_planned_step_names(planned_spans):\n",
    "    return [normalize_span_name(s) for s in planned_spans or []]\n",
    "\n",
    "def aggregate_severity(failures):\n",
    "    if \"F13_INCOMPLETE\" in failures:\n",
    "        return \"blocker\"\n",
    "    if \"F1_FINAL_INCORRECT\" in failures:\n",
    "        return \"critical\"\n",
    "    if failures:\n",
    "        return \"minor\"\n",
    "    return \"none\"\n",
    "\n",
    "def run_phase3_task_eval(dataset, outputs):\n",
    "    output_lookup = {o[\"task_id\"]: o for o in outputs}\n",
    "    rows = []\n",
    "\n",
    "    for task in dataset:\n",
    "        task_id = task[\"task_id\"]\n",
    "        planned_steps = extract_planned_step_names(task[\"planned_spans\"])\n",
    "        expected_answer = task[\"expected_answer\"]\n",
    "\n",
    "        # üîí HARD GUARD: output must exist\n",
    "        if task_id not in output_lookup:\n",
    "            raise KeyError(f\"No output found for task_id: {task_id}\")\n",
    "\n",
    "        output = output_lookup[task_id]\n",
    "        failures = []\n",
    "\n",
    "        # ‚úÖ Accept either key\n",
    "        final_answer = (\n",
    "            output.get(\"final_answer\")\n",
    "            or output.get(\"answer\")\n",
    "        )\n",
    "\n",
    "        if not final_answer:\n",
    "            failures.append(\"F13_INCOMPLETE\")\n",
    "            rows.append({\n",
    "                \"task_id\": task_id,\n",
    "                \"planned_steps\": planned_steps,\n",
    "                \"final_answer\": None,\n",
    "                \"answer_correct\": False,\n",
    "                \"failures\": failures,\n",
    "                \"severity\": aggregate_severity(failures)\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        answer_correct = final_answer == expected_answer\n",
    "\n",
    "        if not answer_correct:\n",
    "            failures.append(\"F1_FINAL_INCORRECT\")\n",
    "\n",
    "        rows.append({\n",
    "            \"task_id\": task_id,\n",
    "            \"planned_steps\": planned_steps,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"answer_correct\": answer_correct,\n",
    "            \"failures\": failures,\n",
    "            \"severity\": aggregate_severity(failures)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "77e7145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Span-level evaluation\n",
    "# -----------------------\n",
    "\n",
    "# -----------------------\n",
    "# Span-level evaluation (FIXED FIELD NAMES)\n",
    "# -----------------------\n",
    "\n",
    "def run_phase3_span_eval(dataset, outputs):\n",
    "    rows = []\n",
    "    output_lookup = {o[\"task_id\"]: o for o in outputs}\n",
    "\n",
    "    for task in dataset:\n",
    "        task_id = task[\"task_id\"]\n",
    "        planned_steps = task.get(\"planned_spans\", [])\n",
    "\n",
    "        output = output_lookup.get(task_id, {})\n",
    "        actual_steps = output.get(\"trajectory\", [])\n",
    "\n",
    "        planned_set = set(planned_steps)\n",
    "        actual_set = set(actual_steps)\n",
    "\n",
    "        missing_steps = sorted(planned_set - actual_set)\n",
    "\n",
    "        span_failures = []\n",
    "        if missing_steps:\n",
    "            span_failures.append(\"F8_MISSING_STEP\")\n",
    "\n",
    "        rows.append({\n",
    "            \"task_id\": task_id,\n",
    "            \"planned_steps\": planned_steps,\n",
    "            \"actual_steps\": actual_steps,\n",
    "            \"missing_steps\": missing_steps,\n",
    "            \"span_failures\": span_failures\n",
    "        })\n",
    "\n",
    "    # üîí FORCE schema so display(df) NEVER fails\n",
    "    return pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"task_id\",\n",
    "            \"planned_steps\",\n",
    "            \"actual_steps\",\n",
    "            \"missing_steps\",\n",
    "            \"span_failures\"\n",
    "        ]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb78468",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b0660b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Task-Level Evaluation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>question</th>\n",
       "      <th>planned_steps</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>final_answer</th>\n",
       "      <th>answer_correct</th>\n",
       "      <th>failures</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>task_correct_1</td>\n",
       "      <td>Assess the risk of a shipment delay for lane MEM ‚Üí ATL.</td>\n",
       "      <td>[Understand lane and shipment context, Review historical transit performance, Assess external risk factors, Summarize delay risk]</td>\n",
       "      <td>The shipment has a low to moderate risk of delay, with occasional congestion but generally stable transit performance.</td>\n",
       "      <td>The shipment has a low to moderate risk of delay, with occasional congestion but generally stable transit performance.</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>task_correct_2</td>\n",
       "      <td>Evaluate inventory risk for SKU A123 at the Memphis distribution center.</td>\n",
       "      <td>[Understand inventory context, Review demand patterns, Assess replenishment constraints, Summarize inventory risk]</td>\n",
       "      <td>SKU A123 has a moderate inventory risk driven by demand variability and longer-than-average replenishment lead times.</td>\n",
       "      <td>SKU A123 has a moderate inventory risk driven by demand variability and longer-than-average replenishment lead times.</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>task_incorrect_1</td>\n",
       "      <td>Assess the risk of a shipment delay for lane ORD ‚Üí JFK.</td>\n",
       "      <td>[Understand lane and shipment context, Review historical transit performance, Assess external risk factors, Summarize delay risk]</td>\n",
       "      <td>The shipment has a moderate to high risk of delay due to airport congestion at JFK and frequent weather disruptions.</td>\n",
       "      <td>The shipment has a low risk of delay since most lanes operate reliably.</td>\n",
       "      <td>False</td>\n",
       "      <td>[F1_FINAL_INCORRECT]</td>\n",
       "      <td>critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>task_incorrect_missing_step</td>\n",
       "      <td>Evaluate the risk of a stockout for SKU B456 during peak season.</td>\n",
       "      <td>[Understand product and seasonality, Review demand forecast, Assess supply constraints, Summarize stockout risk]</td>\n",
       "      <td>SKU B456 has a high stockout risk due to forecast uncertainty, demand surges, and constrained supplier capacity.</td>\n",
       "      <td>Based on forecasted demand, the stockout risk appears low.</td>\n",
       "      <td>False</td>\n",
       "      <td>[F1_FINAL_INCORRECT]</td>\n",
       "      <td>critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>task_incorrect_3</td>\n",
       "      <td>Assess the impact of a supplier disruption in Southeast Asia.</td>\n",
       "      <td>[Understand disruption context, Identify affected supply, Assess mitigation options, Summarize impact]</td>\n",
       "      <td>The disruption poses a high supply risk, potentially extending lead times and increasing downstream service failures.</td>\n",
       "      <td>The disruption mainly increases transportation and sourcing costs.</td>\n",
       "      <td>False</td>\n",
       "      <td>[F1_FINAL_INCORRECT]</td>\n",
       "      <td>critical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       task_id  \\\n",
       "0               task_correct_1   \n",
       "1               task_correct_2   \n",
       "2             task_incorrect_1   \n",
       "3  task_incorrect_missing_step   \n",
       "4             task_incorrect_3   \n",
       "\n",
       "                                                                   question  \\\n",
       "0                   Assess the risk of a shipment delay for lane MEM ‚Üí ATL.   \n",
       "1  Evaluate inventory risk for SKU A123 at the Memphis distribution center.   \n",
       "2                   Assess the risk of a shipment delay for lane ORD ‚Üí JFK.   \n",
       "3          Evaluate the risk of a stockout for SKU B456 during peak season.   \n",
       "4             Assess the impact of a supplier disruption in Southeast Asia.   \n",
       "\n",
       "                                                                                                                       planned_steps  \\\n",
       "0  [Understand lane and shipment context, Review historical transit performance, Assess external risk factors, Summarize delay risk]   \n",
       "1                 [Understand inventory context, Review demand patterns, Assess replenishment constraints, Summarize inventory risk]   \n",
       "2  [Understand lane and shipment context, Review historical transit performance, Assess external risk factors, Summarize delay risk]   \n",
       "3                   [Understand product and seasonality, Review demand forecast, Assess supply constraints, Summarize stockout risk]   \n",
       "4                             [Understand disruption context, Identify affected supply, Assess mitigation options, Summarize impact]   \n",
       "\n",
       "                                                                                                          expected_answer  \\\n",
       "0  The shipment has a low to moderate risk of delay, with occasional congestion but generally stable transit performance.   \n",
       "1   SKU A123 has a moderate inventory risk driven by demand variability and longer-than-average replenishment lead times.   \n",
       "2    The shipment has a moderate to high risk of delay due to airport congestion at JFK and frequent weather disruptions.   \n",
       "3        SKU B456 has a high stockout risk due to forecast uncertainty, demand surges, and constrained supplier capacity.   \n",
       "4   The disruption poses a high supply risk, potentially extending lead times and increasing downstream service failures.   \n",
       "\n",
       "                                                                                                             final_answer  \\\n",
       "0  The shipment has a low to moderate risk of delay, with occasional congestion but generally stable transit performance.   \n",
       "1   SKU A123 has a moderate inventory risk driven by demand variability and longer-than-average replenishment lead times.   \n",
       "2                                                 The shipment has a low risk of delay since most lanes operate reliably.   \n",
       "3                                                              Based on forecasted demand, the stockout risk appears low.   \n",
       "4                                                      The disruption mainly increases transportation and sourcing costs.   \n",
       "\n",
       "   answer_correct              failures  severity  \n",
       "0            True                    []      none  \n",
       "1            True                    []      none  \n",
       "2           False  [F1_FINAL_INCORRECT]  critical  \n",
       "3           False  [F1_FINAL_INCORRECT]  critical  \n",
       "4           False  [F1_FINAL_INCORRECT]  critical  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Span-Level Evaluation\n",
      "üìå Span-Level Evaluation (Reason Diagnostics)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>planned_steps</th>\n",
       "      <th>actual_steps</th>\n",
       "      <th>missing_steps</th>\n",
       "      <th>span_failures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>task_correct_1</td>\n",
       "      <td>[Understand lane and shipment context, Review historical transit performance, Assess external risk factors, Summarize delay risk]</td>\n",
       "      <td>[Understand lane and shipment context, Review historical transit performance, Assess external risk factors, Summarize delay risk]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>task_correct_2</td>\n",
       "      <td>[Understand inventory context, Review demand patterns, Assess replenishment constraints, Summarize inventory risk]</td>\n",
       "      <td>[Understand inventory context, Review demand patterns, Assess replenishment constraints, Summarize inventory risk]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>task_incorrect_1</td>\n",
       "      <td>[Understand lane and shipment context, Review historical transit performance, Assess external risk factors, Summarize delay risk]</td>\n",
       "      <td>[Make general assumption, Summarize delay risk]</td>\n",
       "      <td>[Assess external risk factors, Review historical transit performance, Understand lane and shipment context]</td>\n",
       "      <td>[F8_MISSING_STEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>task_incorrect_missing_step</td>\n",
       "      <td>[Understand product and seasonality, Review demand forecast, Assess supply constraints, Summarize stockout risk]</td>\n",
       "      <td>[Understand product and seasonality, Review demand forecast, Summarize stockout risk]</td>\n",
       "      <td>[Assess supply constraints]</td>\n",
       "      <td>[F8_MISSING_STEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>task_incorrect_3</td>\n",
       "      <td>[Understand disruption context, Identify affected supply, Assess mitigation options, Summarize impact]</td>\n",
       "      <td>[Focus on cost impact]</td>\n",
       "      <td>[Assess mitigation options, Identify affected supply, Summarize impact, Understand disruption context]</td>\n",
       "      <td>[F8_MISSING_STEP]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       task_id  \\\n",
       "0               task_correct_1   \n",
       "1               task_correct_2   \n",
       "2             task_incorrect_1   \n",
       "3  task_incorrect_missing_step   \n",
       "4             task_incorrect_3   \n",
       "\n",
       "                                                                                                                       planned_steps  \\\n",
       "0  [Understand lane and shipment context, Review historical transit performance, Assess external risk factors, Summarize delay risk]   \n",
       "1                 [Understand inventory context, Review demand patterns, Assess replenishment constraints, Summarize inventory risk]   \n",
       "2  [Understand lane and shipment context, Review historical transit performance, Assess external risk factors, Summarize delay risk]   \n",
       "3                   [Understand product and seasonality, Review demand forecast, Assess supply constraints, Summarize stockout risk]   \n",
       "4                             [Understand disruption context, Identify affected supply, Assess mitigation options, Summarize impact]   \n",
       "\n",
       "                                                                                                                        actual_steps  \\\n",
       "0  [Understand lane and shipment context, Review historical transit performance, Assess external risk factors, Summarize delay risk]   \n",
       "1                 [Understand inventory context, Review demand patterns, Assess replenishment constraints, Summarize inventory risk]   \n",
       "2                                                                                    [Make general assumption, Summarize delay risk]   \n",
       "3                                              [Understand product and seasonality, Review demand forecast, Summarize stockout risk]   \n",
       "4                                                                                                             [Focus on cost impact]   \n",
       "\n",
       "                                                                                                 missing_steps  \\\n",
       "0                                                                                                           []   \n",
       "1                                                                                                           []   \n",
       "2  [Assess external risk factors, Review historical transit performance, Understand lane and shipment context]   \n",
       "3                                                                                  [Assess supply constraints]   \n",
       "4       [Assess mitigation options, Identify affected supply, Summarize impact, Understand disruption context]   \n",
       "\n",
       "       span_failures  \n",
       "0                 []  \n",
       "1                 []  \n",
       "2  [F8_MISSING_STEP]  \n",
       "3  [F8_MISSING_STEP]  \n",
       "4  [F8_MISSING_STEP]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Phase 3 ‚Äì Display (FIXED)\n",
    "# -----------------------\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Build lookup from the SAME dataset used in eval\n",
    "task_lookup = {t[\"task_id\"]: t for t in multi_span_dataset}\n",
    "\n",
    "# Re-run task eval using the same dataset\n",
    "df_display = run_phase3_task_eval(\n",
    "    multi_span_dataset,\n",
    "    multi_span_outputs\n",
    ")\n",
    "\n",
    "# Enrich\n",
    "df_display[\"question\"] = df_display[\"task_id\"].map(\n",
    "    lambda x: task_lookup[x][\"question\"]\n",
    ")\n",
    "\n",
    "df_display[\"expected_answer\"] = df_display[\"task_id\"].map(\n",
    "    lambda x: task_lookup[x][\"expected_answer\"]\n",
    ")\n",
    "\n",
    "print(\"üìå Task-Level Evaluation\")\n",
    "\n",
    "display(\n",
    "    df_display[\n",
    "        [\n",
    "            \"task_id\",\n",
    "            \"question\",\n",
    "            \"planned_steps\",\n",
    "            \"expected_answer\",\n",
    "            \"final_answer\",\n",
    "            \"answer_correct\",\n",
    "            \"failures\",\n",
    "            \"severity\"\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"üìå Span-Level Evaluation\")\n",
    "\n",
    "print(\"üìå Span-Level Evaluation (Reason Diagnostics)\")\n",
    "\n",
    "df_phase3_spans = run_phase3_span_eval(\n",
    "    multi_span_dataset,\n",
    "    multi_span_outputs\n",
    ")\n",
    "\n",
    "display(df_phase3_spans)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f6a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a315c9fd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
